\section{Distributed LSH and All-Pairs Computation}
\label{sec:distributed}

\Paragraph{Distributed LSH} The approach of supporting distributed NN search with both LSH and LayerLSH is straightforward. Suppose we have $n$ workers. A bucket with key $bk$ is assigned to worker $\frac{H(bk)}{n}$ where $H(\cdot)$ is any hash function that maps a bucket key to an integer. As a query $q$ arrives, its multiple hash values $G_1(q), G_2(q), \ldots, G_l(q)$ corresponding to multiple hash tables are first calculated. The query is then sent to multiple workers $\frac{H(G_j(q))}{n}, 1\leq j\leq l$ for local computation. The candidates obtained by local NN search are refined before being merged as a global candidate list, where the refining method could be extracting only the top $k$ nearest neighbors. Due to the skewed data distribution, hot spots might exist in distributed LSH, while LayerLSH has the advantage of alleviating hot spot contention.

\begin{comment}
\Paragraph{LSB and LayerLSB} The basic idea of NN search in LSB is bidirectional expansion on an ordered $z$-values list. In distributed implementation, it is intuitive to split the $z$-values list by range partition and let each worker serve for a range of the list. However, hot spots might exist due to skewed data distribution. We assign the dense ranges to additional workers to distribute the heavy workload. Upon receiving a query request, the worker routes the query to additional workers, such that the heavy workload can be shared by redundant workers. Considering the high synchronization cost in distributed processing, a set of local neighbors are returned rather than only one at each time. Similarly, in distributed LayerLSB, the child trees' $z$-values are also split by range partition and allocated to additional workers. Upon receiving a query request, the worker that holds the parent tree first compute its $z$-values with child LSB base, and routes the query to the corresponding workers where the $z$-values are covered.
\end{comment}

\Paragraph{All-Pairs Computation} All-pairs computation is a common preprocessing step in many applications, e.g., retrieving similarity matrix for learning data correlations \cite{allpair1}, pruning distant neighbors for abstracting a graph structure \cite{ap}, evaluating implicit properties for each data point \cite{lshcluster}, and so on. All-pairs computation is known as a computation intensive task, which requires $N^2$ distance measurements. This is extremely costly for large volume and high dimensional data. Since the all-pairs computation is often performed only based on the nearest neighbors in these applications, LSH is an ideal approximation method to optimize all-pairs computation. Furthermore, using distributed machines can further speedup the computation intensive task. The LSH buckets are distributed among multiple workers, where the all-pairs computation is performed locally within each bucket.

However, distributed LSH-based all-pairs computation suffers from the drawback of skewed bucket size distribution. The workers with dense buckets can be the stragglers, which can significantly slow down the whole process. Fortunately, LayerLSH can alleviate this impact by bounding the bucket size, while at the same time guaranteeing the accuracy. Moreover, we merge similar small buckets in order to not only improve the accuracy but also reduce the number of distributed tasks.

\Paragraph{Case Study: Point Density Evaluation}
We take point density evaluation as a use case for illustration. A point $p_i$'s density $\rho_i$ is defined as the number of neighbors within a radius $R$, i.e., $\rho_i=|\{p_j|\forall j,|p_i,p_j|\leq R\}|$. In this problem, the computation of $\rho_i$ only depends on its nearest neighbors with distance to $p_i$ less than $R$. Suppose the approximated density is $\hat\rho_i$. By using LSH, the probability $\text{Pr}(\hat\rho_i=\rho_i)$ can be studied as follows.

\begin{lemma}
%
\label{lm:prob1}
%
Given a point $p_i$ and an LSH function $h(p_i)=\lfloor \frac{a\cdot
p_i+b}{w}\rfloor$, the probability that $p_i$ and its nearest neighbors set $\{p_j|\forall j, |p_i,p_j|\leq R\}$ are hashed to the same bucket is:
%
\begin{equation}\label{eq:lm:prob1}
%
\begin{aligned}
%
  \emph{Pr}\big[h(p_i)=h(p_{j})|\forall j, |p_i,p_j|\leq R] \geq 1-\frac{4R}{\sqrt{2\pi}w}.
%
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}

Let us consider a number line, where each point is a real number.
$y_i=a\cdot p_i+b$ is a point on the number line. By floor dividing
$w$, the number line is divided into a sequence of $w$-width slots.
According to the LSH function, all the points in the same $w$-width
slot share the the same hash key.  The points that are close to $p_i$
are all hashed to the positions close to $y_i$ on the number line.  The
position of $y_i$ is important.  If $y_i$ is close to the center of
the slot, it is more likely that all $R$-length neighbors of $p_i$
are in the same slot.

According to the definition of $p$-stable distribution
\cite{datar}, given a $d$-dimensional random
vector $a$ each entry of which is chosen independently from a standard
gaussian distribution $\mathcal{N}(0,1)$, for two points $p_i$ and
$p_j$, the distance between their projections $|a\cdot p_i-a\cdot
p_j|$ (here $|\cdot|$ means the absolute value) is distributed as $|p_i, p_j|\cdot x$, where $x$ is the \textbf{absolute
value} of a standard gaussian random variable. Therefore, for any
$p_j$ where $|p_i, p_j|<R$, we have $\max_{j}|y_i-y_j|=\max_{j}|a\cdot
p_i-a\cdot p_j|<R\cdot x$.
%
% Furthermore, since $b$ is drawn uniformly from $[0,w)$,

Moreover, $y_i=a\cdot p_i+b$ is uniformly distributed in a certain
slot. To ensure that $y_i$ and all its $R$-length neighbors are in
the same slot, $y_i$ has to be located in the interval of $[\alpha
w+Rx,(\alpha+1)w-Rx)$ for some $\alpha$.   The probability that $y_i$ resides in such an
interval is $\frac{w-2Rx}{w}=1-\frac{2Rx}{w}$.
%
The probability density function of the absolute value of the standard
gaussian distribution is $f_p(x)=\frac{2e^{-x^2/2}}{\sqrt{2\pi}}$,
where $x \geq 0$.  Therefore, the probability becomes
$1-\frac{2Rx}{w}=\int_{0}^{\infty}(1-\frac{2Rx}{w})f_p(x)dx$, and
a further calculation shows that the probability is
$1-\frac{4R}{\sqrt{2\pi}w}$.

\end{proof}

By applying the LSH properties that are described in Equation (\ref{eq:prob1}) and Equation (\ref{eq:prob2}), we have the following theorem.

\begin{theorem}
\label{the:prob3}
With $l$ groups of $m$ hash functions, the probability is finally enlarged as
\begin{equation}\label{eq:lm:prob3}
\emph{Pr}[\hat\rho_i=\rho_i]\geq 1-\Big[1-\Big(1-\frac{4R}{\sqrt{2\pi}w}\Big)^m\Big]^l.
\end{equation}
\end{theorem}

\begin{comment}
\begin{proof}
After applying $l$ groups of $m$ hash functions, we will obtain $l$ $\hat\rho_i^g$ values $(1\leq g\leq l)$. According to the definition of $\rho_i$, we have
$\hat\rho_i^g \leq \max_{g}\hat\rho_i^g \leq \hat\rho_i$. If
$\max_{g}\hat\rho_i^g \neq \rho_i$, then
$\hat\rho_i^g\neq \rho_i$ for all $1\leq g\leq l$.

From Lemma 1, under a single hash function the probability that $p_i$ and all its $R$-length neighbors are hashed to the same bucket is at least $1-\frac{4R}{\sqrt{2\pi}w}$. With a group of $m$ LSH functions $G=(h_1,h_2,\ldots,h_{m})$ applied on each point, only points sharing all the $m$ hash values are placed in the same partition. Suppose $\hat\rho_i^g$ is the approximated density value for a specific hash function group $G_g(p_i)$. Due to the fact that each LSH function is independently and randomly selected, we have:
\begin{equation}\label{eq:probx}
\begin{aligned}
  \text{Pr}[\hat\rho_i^g=\rho_i]=&\text{Pr}\big[G_g(p_i)=G_g(p_{j})|\forall j, ||p_i,p_j||\leq R\big]\\
  =&\prod_{t=1}^{m}\text{Pr}\big[h_t(p_i)=h_t(p_j)|\forall j, ||p_i,p_j||\leq R\big]\\
  \geq &\Big(1-\frac{4R}{\sqrt{2\pi}w}\Big)^m\notag
\end{aligned}
\end{equation}
Further, since the $l$ groups of hash functions
$G_g(1\leq g\leq l)$ is independently and randomly generated,
we have the following:
\begin{equation}\label{eq:prob3}
\begin{aligned}
  \text{Pr}[\hat\rho_i=\rho_i]=&1-\prod_{g=1}^l\Big(1-\text{Pr}\Big[\hat\rho_i^g=\rho_i\Big]\Big)\\
  \geq &1-\Big[1-\Big(1-\frac{4R}{\sqrt{2\pi}w}\Big)^m\Big]^l\notag
\end{aligned}
\end{equation}
\end{proof}
\end{comment}

Therefore, users are allowed to specify an expected accuracy in density approximation. However, the unbalanced buckets allocation brings troubles in distributed computing. As shown in Section \ref{sec:expr:allpair}), one or two stragglers significantly slow down the whole process. LayerLSH rehashes the overloaded buckets to alleviate this problem. Meanwhile, the theoretical accuracy can be guaranteed by choosing child LSH parameters in terms of Proposition \ref{prop:accuracy}.

%Furthermore, our repartition strategy will not impact the approximation accuracy as depicted in Algorithm \ref{alg:param}.


