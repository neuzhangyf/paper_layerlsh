\section{Preliminaries}
\label{sec:pre}

The problem of nearest neighbors search refers to finding objects that are similar to the query object. Our goal is to design an indexing scheme for approximate $k$NN queries with both high search quality and high efficiency. The typical $k$NN search problem is formally defined as follows.

\begin{definition}
\label{def:knn}
(\textbf{$k$NN}) Given an object $q$, a dataset $O$ and an integer $k$ ($k<|O|$), the $k$NN query returns a set of $k$ objects from $O$ denoted as $\text{KNN}(q,O)$, such that $\forall o\in \text{KNN}(q,O)$, $\forall o'\in O-\text{KNN}(q,O)$, $|q,o|\leq|q,o'|$, where $|\cdot,\cdot|$ denotes the distance between two objects.
\end{definition}

%In this paper, our goal is to design an indexing scheme for approximate $k$NN queries with both high search quality and high efficiency.
%For approximate $k$NN query, we aim to find $k$ objects whose distances are within a small factor $(1+\epsilon)$ of the exact $k$-nearest neighbors' distances and minimize $\epsilon$. At the same time, we also aim to improve the efficiency and reduce the query cost, e.g., the number of distance measurements or disk I/Os.

The Locality-Sensitive Hashing (LSH) function has the property that points that are closer to each other have a higher probability of colliding than points that are farther apart \cite{orilsh}. Let $O$ be a set of data objects in $d$-dimensional space $\mathbb{R}^d$. The commonly used LSH family for Euclidean distance consists of LSH functions in the following form
\cite{datar}:
%
\begin{equation}\label{eq:lsh}
%
    h(o)=\bigg\lfloor \frac{a\cdot o+b}{w}\bigg\rfloor
%
\end{equation}
%
where $a$ is a $d$-dimensional random vector, each entry of which is
chosen independently from standard Gaussian distribution $\mathcal{N}(0,1)$ \cite{stabledist},
$b$ is a real number chosen from $[0,w]$, and $w$ is also a real
number representing the partition width of the LSH function.

For two data objects $o_1$ and $o_2$, let $s=|o_1,o_2|$. The probability that $o_1$ and $o_2$ collide under a randomly chosen hash function $h$, denoted as $p(s,w)$, can be computed \cite{datar}:
\begin{equation}\label{eq:prob}
p(s,w)=1-2norm(-\frac{w}{s})-\frac{2s}{\sqrt{2\pi}w}(1-e^{-\frac{w^2}{2s^2}}),
\end{equation}
where $norm(\cdot)$ represents the cumulative distribution function for a random variable that is distributed as Gaussian Distribution.

To reduce such
\emph{false positives}, a group of $m$ hash functions
$G(\cdot)=\{h_1(\cdot),h_2(\cdot),\ldots,h_m(\cdot)\}$ are employed. Thus, each object $o$ is labeled with a compound hash key
$G(o)=\{h_1(o),h_2(o),\ldots,h_m(o)\}$, which is the bucket key. Then, the probability that two objects collide is reduced as follows.
\begin{equation}\label{eq:prob1}
%
\begin{aligned}
%
  Pr[G(o_1)=G(o_2)]=\prod_{i=1}^m Pr[h_i(o_1)=h_i(o_2)]=p^m
%
\end{aligned}
%
\end{equation}
In order to reduce the loss of false negatives, a set of $l$ hash groups $\{G_1(\cdot),G_2(\cdot),\ldots,G_l(\cdot)\}$ are employed and $l$ hash tables are constructed, hoping that the close points collide at least on one hash table. The final collision probability $P$ is:
\begin{equation}\label{eq:prob2}
%
\begin{aligned}
%
  P=1-\prod_{i=1}^l \Big\{1-Pr[G_i(o_1)=G_i(o_2)]\Big\}=1-[1-p^m]^l
\end{aligned}
%
\end{equation}
Given a query $q$, a candidate set by the union of $l$ buckets that
query $q$ is hashed to is generated. Then, these candidates are ranked according to their distances to $q$, and finally the top $k$ candidates are returned.
%Note that, the theory foundation relies on a specified radius $r$. To answer a $k$NN query, we can either run repeatedly with different values of $\{r, cr, c^2r, \ldots\}$ or use a single ``magic'' radius to process different queries.
