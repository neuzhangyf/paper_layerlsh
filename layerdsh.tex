\section{LayerDSH: Rebuild DSH}
\label{sec:recdsh}

Our idea should work as long as the basic index structure is a hash table. \textbf{DSH} \cite{Gao:2014:DDS:2588555.2588565} was proposed to address the unbalanced distribution problem, which can also integrate the multi-layered structure to further balance the hash table.

%However, how to guarantee the search quality after integrating multi-layered structure is an open problem for these LSH variants.

\Paragraph{LayerDSH} The DSH family is defined as follows \cite{Gao:2014:DDS:2588555.2588565}:
\begin{definition}
\label{def:dsh}
(\textbf{DSH family}) A family $\mathcal{H}=\{h:R^d\rightarrow\{0,1\}\}$ is called $(k,ck,p_1,p_2)$-sensitive if for any query point $q\in R^d$ and $o\in O$
\begin{itemize}
  \item If $o\in NN(q,k)$ then $\frac{|\{h|h(o)=h(q),h\in\mathcal{H}\}|}{|\mathcal{H}|}\geq p_1$,
  \item If $o\notin NN(q,ck)$ then $\frac{|\{h|h(o)=h(q),h\in\mathcal{H}\}|}{|\mathcal{H}|}\leq p_2$.
\end{itemize}
\end{definition}
The DSH family can be learned from the data and generated by combining adaptive boosting and spectral techniques \cite{Gao:2014:DDS:2588555.2588565}, which is endowed with good theoretical guarantee.

Even though the points are distributed more evenly and the buckets are more balanced in DSH than that in LSH, the buckets are not totally balanced as shown in the experimental results \cite{Gao:2014:DDS:2588555.2588565}. As DSH aims to generate proper hash families to generate balanced hash tables, our idea can be applied to the DSH tables as a postprocessing step to further balance the hash tables. We propose LayerDSH which rehashes the dense buckets and merges the sparse buckets for DSH. The building process of LayerDSH is similar to LayerLSH. According to the definition of DSH, the expected recall $\alpha$ should be set as $\alpha=p_1$ which is defined in Definition \ref{def:dsh}. In addition, to sustain the success probability, the probability $p$ appeared in Proposition \ref{prop:accuracy} and \ref{prop:efficiency} should be set as $p=p_1$ instead of $p=p(r^*,w)$.

\begin{comment}
DSH \cite{Gao:2014:DDS:2588555.2588565} provides data sensitive hash index to accommodate skewed data distribution problem. In this section, we rebuild DSH as a postprocessing step to further improve effectiveness and efficiency.

The DSH lays the theoretical foundation on directly solving the $k$NN problem. The DSH family is defined as follows \cite{Gao:2014:DDS:2588555.2588565}:
\begin{definition}
\label{def:dsh}
(\textbf{DSH family}) A family $\mathcal{H}=\{h:R^d\rightarrow\{0,1\}\}$ is called $(k,ck,p_1,p_2)$-sensitive if for any query point $q\in R^d$ and $o\in O$
\begin{itemize}
  \item If $o\in NN(q,k)$ then $\frac{|\{h|h(o)=h(q),h\in\mathcal{H}\}|}{|\mathcal{H}|}\geq p_1$,
  \item If $o\notin NN(q,ck)$ then $\frac{|\{h|h(o)=h(q),h\in\mathcal{H}\}|}{|\mathcal{H}|}\leq p_2$.
\end{itemize}
\end{definition}
In terms of the definition, $p_1$ reflects the recall and $p_2$ reflects the precision. Note that, the concatenation functions generated using DSH family are still effective. The DSH family can be generated by combining adaptive boosting and spectral techniques \cite{Gao:2014:DDS:2588555.2588565}, which is endowed with good theoretical guarantee.

Since DSH is a table-based LSH index, the building process is similar to LayerLSH. Note that, according to the definition of DSH, the expected recall $\alpha$ should be set as $\alpha=p_1$. In addition, to sustain the success probability, the probability $p$ in Theorem \ref{prop:childparam} should be set as $p=p_1$ instead of $p=p(r^*,w)$.
\end{comment}



