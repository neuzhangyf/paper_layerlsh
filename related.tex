\section{Related Work}
\label{sec:related}

\Paragraph{LSH Variants} The LSH functions based on Euclidean space are proposed by Datar et at. \cite{datar}. Since then, a large number of LSH variants were proposed for improving accuracy and reducing I/O cost, including table-based LSH such as multi-probe LSH \cite{mplsh}, entropy LSH \cite{Panigrahy:2006:EBN:1109557.1109688}, C2LSH \cite{c2lsh}, and tree-based LSH such as LSB-tree \cite{lsb}, LSH Forest \cite{Bawa:2005:LFS:1060745.1060840}, SK-LSH \cite{sklsh}. Besides them, numerous excellent works are proposed in recent years. LazyLSH \cite{Zheng:2016:LAN:2882903.2882930} is able to answer approximate NN queries for multiple $l_p$ metrics. It uses a single base index to support the computations in multiple $l_p$ spaces. QALSH \cite{Huang:2015:QLH:2850469.2850470} introduces a novel concept of query-aware bucket partition which uses a given query as the anchor for bucket partition. This differs from the traditional LSH that partitions buckets ahead without any knowledge of the query distribution. SRS \cite{srs} requires only a single tiny index to answer approximate NN queries with theoretical guarantees.

%\Paragraph{LSH-based Applications} Researchers have leveraged the locality-preserving property of LSH to improve the performance in a wide range of applications. McConville et al. uses LSH to accelerate large scale centroid-based clustering \cite{lshcluster}. Liu et al. exploits LSH for distributed graph summarization \cite{graphsummarize}.

%\Paragraph{Distributed LSH} A set of research works focus on design efficient distributed LSH indices for supporting large data sets. Zhang et al. utilize the locality preserving property of $z$-values and perform $z$-value based partition join in MapReduce to approximate the $k$NN joins \cite{Zhang:2012:EPK:2247596.2247602}. Haghani et al. propose mappings from the multi-dimensional LSH bucket space to the linearly ordered set of peers that jointly maintain the indexed data, so that buckets likely to hold similar data are stored on the same or neighboring peers in a P2P system \cite{Haghani:2009:DSS:1516360.1516446}. Bahmani et al. propose a distributed Entropy LSH implementation and prove that it exponentially decreases the network cost, while maintaining a good load balance between different machines \cite{distlsh}. PLSH \cite{Sundaram:2013:SSS:2556549.2556574} is a parallel LSH that supports high-throughput streaming of new data, which exploits an insert-optimized hash table structure and efficient data expiration algorithm for streaming data.

\Paragraph{Data Dependent Hashing} As discussed in Section \ref{sec:intro}, the recently proposed data sensitive hashing, e.g., DSH \cite{Gao:2014:DDS:2588555.2588565} and selective hashing \cite{Gao:2015:SHC:2783258.2783284}, leverage data distributions. However, rather than learning the optimal hash functions from the skewed data, our approach relies on postprocessing and leverages the density of hash values to reorganize the existing structures. It is orthogonal to the data sensitive hashing. As presented in Section \ref{sec:extension:dsh} and Section \ref{sec:expr:overall}, we can rebuild DSH index as a postprocessing step to further improve performance. HashFile \cite{Zhang:2011:HEI:2004686.2005629} also proposes to recursively partition the dense buckets. However, we use a multi-layered structure to organize the points as a general strategy that also benefits the tree-like LSH indices, which differs from it. The NSH \cite{Park:2015:NH:2850583.2850589} and our work share the same intuition that the limited hash bits should be used to better distinguish nearby items instead of capturing the distances among far apart items. However, NSH aims to devise a new hashing mechanism to achieve this goal, while we propose to reconstruct the existing LSH index structures as a postprocessing step. ``Learning to hash'' \cite{7915742} has recently attracted many research efforts, which uses machine learning techniques to learn hash functions from a specific dataset so that the nearest neighbor search result in the hash coding space is as close as possible to the search result in the original space.




